{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f77189596d7c95",
   "metadata": {},
   "source": [
    "# Ejercicio 11 : Asistente RAG Conversacional\n",
    "\n",
    "## Objetivo de la práctica\n",
    "\n",
    "Construir un asistente que:\n",
    "\n",
    "1. Recibe una pregunta del usuario\n",
    "2. Recupera texto relevante de un corpus (ej. libro de Baeza-Yates)\n",
    "3. Genera una respuesta basada en los documentos encontrados\n",
    "4. Mantiene el historial de conversación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756d113e9d3deff",
   "metadata": {},
   "source": [
    "## Parte 0: Librerías necesarias\n",
    "- openai\n",
    "- faiss-cpu\n",
    "- sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import faiss\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66019e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el modelo de embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e1072aca40b2",
   "metadata": {},
   "source": [
    "## Parte 1: Carga del corpus\n",
    "\n",
    "Aquí se debe cargar el corpus con los documentos en PDF.\n",
    "\n",
    "- Libro de Stanford\n",
    "- Libro BM25\n",
    "- Paper: Marcia Bates (1989). The design of browsing and berrypicking techniques for the online search interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8d90af6f06d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "doc_bates = fitz.open(r\"D:\\Universidad\\8 - Octavo\\Recuperacion de la informacion\\Tareas\\Tarea11\\corpus_bates.pdf\")\n",
    "doc_bm25 = fitz.open(r\"D:\\Universidad\\8 - Octavo\\Recuperacion de la informacion\\Tareas\\Tarea11\\corpus_bm25.pdf\")\n",
    "doc_stand = fitz.open(r\"D:\\Universidad\\8 - Octavo\\Recuperacion de la informacion\\Tareas\\Tarea11\\corpus_standfoard.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333f3666484f4ce",
   "metadata": {},
   "source": [
    "## Parte 2: Procesamiento del Corpus\n",
    "\n",
    "Aquí se debe obtener el corpus procesado. El corpus estará formado por documentos que corresponden a las secciones (o subsecciones) de los libros. Cada documento debe indicar a qué libro corresponde, así como las páginas en las que está dentro de ese libro.\n",
    "\n",
    "Recuerden que los documentos procesados no deben contener docos o caracteres ajenos al tema del que tratan.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb82f4c1d92ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saber si el PDF tiene una tabla de contenidos estructurada\n",
    "def check_pdf_toc(doc):\n",
    "    toc = doc.get_toc()\n",
    "    if toc:\n",
    "        print(f\"El PDF tiene una tabla de contenidos estructurada con {len(toc)} entradas.\")\n",
    "    else:\n",
    "        print(\"El PDF no tiene tabla de contenidos estructurada.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db682460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_pdf_toc(doc_bates)\n",
    "#check_pdf_toc(doc_stand)\n",
    "#check_pdf_toc(doc_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "605c1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para extraer texto de un documento PDF\n",
    "def extract_text(doc):\n",
    "    data = []\n",
    "    for i, page in enumerate(doc):\n",
    "        text = page.get_text().encode('utf-8')\n",
    "        if text.strip():\n",
    "            data.append({\n",
    "                \"page\": i + 1,\n",
    "                \"raw\": str(text).strip()\n",
    "            })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13293022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraer texto de los documentos\n",
    "pages_doc_bates = extract_text(doc_bates)\n",
    "pages_doc_bm25 = extract_text(doc_bm25)\n",
    "pages_doc_stand  = extract_text(doc_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5ae8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrames para cada documento\n",
    "df_bates = pd.DataFrame(pages_doc_bates)\n",
    "df_bm25 = pd.DataFrame(pages_doc_bm25)\n",
    "df_stand = pd.DataFrame(pages_doc_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ef834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostrar los DataFrames\n",
    "#df_bates\n",
    "#df_bm25 \n",
    "#df_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f1bf06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto(text):\n",
    "    \"\"\"Limpia el texto extraído de caracteres no deseados\"\"\"\n",
    "    # Remover el prefijo b' y el sufijo '\n",
    "    if text.startswith(\"b'\") and text.endswith(\"'\"):\n",
    "        text = text[2:-1]\n",
    "    \n",
    "    # Reemplazar secuencias de escape\n",
    "    text = text.replace('\\\\n', '\\n').replace('\\\\t', '\\t').replace('\\\\r', '\\r')\n",
    "    \n",
    "    # Remover caracteres especiales pero mantener puntuación básica\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\-\\(\\)\\[\\]\\\"\\'\\/\\%\\&\\*\\+\\=\\<\\>\\@\\#\\$]', '', text)\n",
    "    \n",
    "    # Normalizar espacios en blanco\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca642a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_by_paragraphs(text, min_length=200, max_length=1500):\n",
    "    \"\"\"Divide el texto en párrafos cuando no se detectan secciones\"\"\"\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    sections = []\n",
    "    current_section = \"\"\n",
    "    section_num = 1\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        paragraph = paragraph.strip()\n",
    "        if not paragraph:\n",
    "            continue\n",
    "        \n",
    "        # Si agregar este párrafo excede el límite máximo, crear nueva sección\n",
    "        if len(current_section) + len(paragraph) > max_length and len(current_section) > min_length:\n",
    "            sections.append({\n",
    "                'title': f'Sección {section_num}',\n",
    "                'content': current_section.strip(),\n",
    "                'start_pos': 0,  # Aproximado\n",
    "                'end_pos': 0     # Aproximado\n",
    "            })\n",
    "            current_section = paragraph\n",
    "            section_num += 1\n",
    "        else:\n",
    "            current_section += \"\\n\\n\" + paragraph if current_section else paragraph\n",
    "    \n",
    "    # Agregar la última sección\n",
    "    if current_section.strip() and len(current_section) > min_length:\n",
    "        sections.append({\n",
    "            'title': f'Sección {section_num}',\n",
    "            'content': current_section.strip(),\n",
    "            'start_pos': 0,\n",
    "            'end_pos': 0\n",
    "        })\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8026a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_secciones(text):\n",
    "    # Patrones para detectar títulos de secciones\n",
    "    section_patterns = [\n",
    "        r'\\n\\s*\\d+\\.\\s*[A-Z][^\\n]*\\n',           # 1. Título\n",
    "        r'\\n\\s*\\d+\\.\\d+\\s*[A-Z][^\\n]*\\n',       # 1.1 Subtítulo\n",
    "        r'\\n\\s*\\d+\\.\\d+\\.\\d+\\s*[A-Z][^\\n]*\\n', # 1.1.1 Sub-subtítulo\n",
    "        r'\\n\\s*[A-Z][A-Z\\s]{2,}\\n',              # TÍTULOS EN MAYÚSCULAS\n",
    "        r'\\n\\s*[A-Z][^.\\n]*\\n\\s*\\n',            # Título seguido de línea vacía\n",
    "        r'\\n\\s*Chapter\\s+\\d+[^\\n]*\\n',          # Chapter X\n",
    "        r'\\n\\s*Section\\s+\\d+[^\\n]*\\n',          # Section X\n",
    "        r'\\n\\s*Abstract\\s*\\n',                   # Abstract\n",
    "        r'\\n\\s*Introduction\\s*\\n',               # Introduction\n",
    "        r'\\n\\s*Conclusion\\s*\\n',                 # Conclusion\n",
    "        r'\\n\\s*References\\s*\\n',                 # References\n",
    "        r'\\n\\s*Bibliography\\s*\\n',               # Bibliography\n",
    "    ]\n",
    "    \n",
    "    sections = []\n",
    "    current_pos = 0\n",
    "    \n",
    "    # Buscar todos los patrones\n",
    "    all_matches = []\n",
    "    for pattern in section_patterns:\n",
    "        matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "        for match in matches:\n",
    "            all_matches.append((match.start(), match.end(), match.group().strip()))\n",
    "    \n",
    "    # Ordenar por posición\n",
    "    all_matches.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Si no se encuentran secciones, dividir por párrafos largos\n",
    "    if not all_matches:\n",
    "        return divide_by_paragraphs(text)\n",
    "    \n",
    "    # Dividir el texto en secciones\n",
    "    for i, (start, end, title) in enumerate(all_matches):\n",
    "        # Contenido de la sección anterior\n",
    "        if i == 0:\n",
    "            # Contenido antes de la primera sección\n",
    "            if start > 0:\n",
    "                content = text[current_pos:start].strip()\n",
    "                if len(content) > 100:\n",
    "                    sections.append({\n",
    "                        'title': 'Introducción/Preámbulo',\n",
    "                        'content': content,\n",
    "                        'start_pos': current_pos,\n",
    "                        'end_pos': start\n",
    "                    })\n",
    "        \n",
    "        # Encontrar el final de esta sección\n",
    "        if i < len(all_matches) - 1:\n",
    "            section_end = all_matches[i + 1][0]\n",
    "        else:\n",
    "            section_end = len(text)\n",
    "        \n",
    "        # Contenido de la sección actual\n",
    "        section_content = text[end:section_end].strip()\n",
    "        if len(section_content) > 100:\n",
    "            sections.append({\n",
    "                'title': title,\n",
    "                'content': section_content,\n",
    "                'start_pos': end,\n",
    "                'end_pos': section_end\n",
    "            })\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0af2d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_page_range(text_position, total_text_length, total_pages):\n",
    "    \"\"\"Estima el rango de páginas basado en la posición del texto\"\"\"\n",
    "    if total_text_length == 0:\n",
    "        return 1, 1\n",
    "    \n",
    "    # Calcular página aproximada\n",
    "    page_ratio = text_position / total_text_length\n",
    "    estimated_page = max(1, int(page_ratio * total_pages))\n",
    "    \n",
    "    return estimated_page, estimated_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77f41395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents_with_sections(df_list, doc_names):\n",
    "    \"\"\"Procesa los DataFrames y divide el contenido en secciones\"\"\"\n",
    "    processed_docs = []\n",
    "    \n",
    "    for df, doc_name in zip(df_list, doc_names):\n",
    "        print(f\"Procesando {doc_name}...\")\n",
    "        \n",
    "        # Concatenar todo el texto del documento\n",
    "        full_text = \"\"\n",
    "        page_texts = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            clean_content = limpiar_texto(row['raw'])\n",
    "            if len(clean_content) > 50:  # Filtrar páginas casi vacías\n",
    "                page_texts.append((row['page'], clean_content))\n",
    "                full_text += f\"\\n\\n--- PÁGINA {row['page']} ---\\n\\n\" + clean_content\n",
    "        \n",
    "        print(f\"Texto completo: {len(full_text)} caracteres\")\n",
    "        \n",
    "        # Detectar secciones en el texto completo\n",
    "        sections = dividir_secciones(full_text)\n",
    "        print(f\"Secciones detectadas: {len(sections)}\")\n",
    "        \n",
    "        # Crear documentos por sección\n",
    "        for i, section in enumerate(sections):\n",
    "            # Estimar páginas basándose en la posición del texto\n",
    "            start_page, end_page = estimate_page_range(\n",
    "                section['start_pos'], \n",
    "                len(full_text), \n",
    "                len(page_texts)\n",
    "            )\n",
    "            \n",
    "            # Buscar páginas más precisas basándose en el contenido\n",
    "            actual_pages = []\n",
    "            section_content = section['content']\n",
    "            \n",
    "            for page_num, page_content in page_texts:\n",
    "                # Si hay overlap significativo entre el contenido de la sección y la página\n",
    "                overlap = len(set(section_content.split()) & set(page_content.split()))\n",
    "                if overlap > 10:  # Umbral de overlap\n",
    "                    actual_pages.append(page_num)\n",
    "            \n",
    "            if actual_pages:\n",
    "                start_page, end_page = min(actual_pages), max(actual_pages)\n",
    "            \n",
    "            doc = {\n",
    "                'id': f\"{doc_name}_section_{i+1}\",\n",
    "                'content': section['content'],\n",
    "                'source': doc_name,\n",
    "                'section_title': section['title'],\n",
    "                'section_number': i + 1,\n",
    "                'page_start': start_page,\n",
    "                'page_end': end_page,\n",
    "                'length': len(section['content']),\n",
    "                'pages': actual_pages if actual_pages else [start_page]\n",
    "            }\n",
    "            processed_docs.append(doc)\n",
    "        \n",
    "        print(f\"✓ {doc_name}: {len([d for d in processed_docs if d['source'] == doc_name])} secciones procesadas\")\n",
    "    \n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03faefd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Bates_1989...\n",
      "Texto completo: 54296 caracteres\n",
      "Secciones detectadas: 18\n",
      "✓ Bates_1989: 18 secciones procesadas\n",
      "Procesando BM25_Book...\n",
      "Texto completo: 105009 caracteres\n",
      "Secciones detectadas: 22\n",
      "✓ BM25_Book: 22 secciones procesadas\n",
      "Procesando Stanford_Book...\n",
      "Texto completo: 1361638 caracteres\n",
      "Secciones detectadas: 3\n",
      "✓ Stanford_Book: 3 secciones procesadas\n"
     ]
    }
   ],
   "source": [
    "document_names = ['Bates_1989', 'BM25_Book', 'Stanford_Book']\n",
    "processed_documents = process_documents_with_sections([df_bates, df_bm25, df_stand], document_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0264565d37ac35",
   "metadata": {},
   "source": [
    "## Parte 3: Cálculo de Embeddings e Indexación en base de datos vectorial\n",
    "\n",
    "Aquí, una vez que se ha calculado el embedding de cada documento, se deberá indexar este embedding en una base de datos vectorial como FAISS, ChromaDB o Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8325a0116779dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(documents):\n",
    "    # Extraer el contenido de texto\n",
    "    texts = [doc['content'] for doc in documents]\n",
    "    \n",
    "    # Calcular embeddings usando sentence-transformers\n",
    "    embeddings = embedding_model.encode(texts, batch_size=32)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac151686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"Crea índice FAISS para búsqueda vectorial\"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # Crear índice FAISS usando Inner Product (para cosine similarity)\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    \n",
    "    # Normalizar embeddings para cosine similarity\n",
    "    embeddings_normalized = embeddings.copy()\n",
    "    faiss.normalize_L2(embeddings_normalized)\n",
    "    \n",
    "    # Agregar embeddings al índice\n",
    "    index.add(embeddings_normalized.astype('float32'))\n",
    "    \n",
    "    print(f\"✓ Índice FAISS creado con {index.ntotal} documentos\")\n",
    "    return index, embeddings_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46d7dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = create_embeddings(processed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba1855ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Índice FAISS creado con 43 documentos\n"
     ]
    }
   ],
   "source": [
    "faiss_index, normalized_embeddings = create_faiss_index(document_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02916370ab201c1",
   "metadata": {},
   "source": [
    "## Parte 4: Búsqueda y obtención del contexto\n",
    "\n",
    "En esta parte debemos definir una _query_ y buscar los documentos que más se relacionan con ella.\n",
    "\n",
    "Estos documentos formarán el contexto que vamos a entregar al LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ccd438b201b197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_documents(query: str, index, embeddings, documents, top_k=3):\n",
    "    \"\"\"\n",
    "    Busca documentos similares a la query usando FAISS\n",
    "    \"\"\"\n",
    "    # Calcular embedding de la query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Normalizar el embedding de la query\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Buscar documentos similares\n",
    "    scores, indices = index.search(query_embedding.astype('float32'), top_k)\n",
    "    \n",
    "    # Preparar resultados\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        doc = documents[idx]\n",
    "        result = {\n",
    "            'score': float(score),\n",
    "            'document': doc,\n",
    "            'content': doc['content'],\n",
    "            'source': f\"{doc['source']}, páginas {doc['pages']}\"\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d65842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context(query: str, results: List[Dict], max_length: int = 4000) -> str:\n",
    "    context_parts = []\n",
    "    total_length = 0\n",
    "    \n",
    "    # Agregar la query\n",
    "    context_parts.append(f\"Pregunta: {query}\\n\")\n",
    "    context_parts.append(\"\\nContexto relevante de los documentos:\\n\")\n",
    "    \n",
    "    # Ordenar resultados por score de similitud\n",
    "    sorted_results = sorted(results, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Agregar cada documento encontrado\n",
    "    for i, result in enumerate(sorted_results, 1):\n",
    "        doc = result['document']\n",
    "        \n",
    "        # Crear encabezado del documento\n",
    "        header = f\"\\nFuente {i} ({result['score']:.3f}): {doc['source']}\"\n",
    "        header += f\"\\nSección: {doc['section_title']}\"\n",
    "        header += f\"\\nPáginas: {', '.join(map(str, doc['pages']))}\"\n",
    "        \n",
    "        # Calcular longitud del contenido\n",
    "        content = f\"Contenido:\\n{result['content']}\\n\"\n",
    "        section_length = len(header) + len(content)\n",
    "        \n",
    "        # Verificar si agregar esta sección excedería el límite\n",
    "        if total_length + section_length > max_length:\n",
    "            content = content[:max_length - total_length - len(header) - 20] + \"...\"\n",
    "        \n",
    "        context_parts.append(header)\n",
    "        context_parts.append(content)\n",
    "        \n",
    "        total_length += len(header) + len(content)\n",
    "        \n",
    "        # Detener si alcanzamos el límite de longitud\n",
    "        if total_length >= max_length:\n",
    "            context_parts.append(\"\\n[Contexto truncado por longitud máxima]\")\n",
    "            break\n",
    "    \n",
    "    return \"\\n\".join(context_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9cf04d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para ejecutar una búsqueda completa\n",
    "def run_search(query: str, index, embeddings, documents, top_k=3):\n",
    "    \"\"\"\n",
    "    Ejecuta una búsqueda completa y prepara el contexto\n",
    "    \"\"\"\n",
    "    # Buscar documentos similares\n",
    "    search_results = search_similar_documents(query, index, embeddings, documents, top_k)\n",
    "    \n",
    "    # Preparar contexto\n",
    "    context = prepare_context(query, search_results)\n",
    "    \n",
    "    return context, search_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158cb4c44f6966f",
   "metadata": {},
   "source": [
    "## Parte 5: Generación de Respuesta\n",
    "\n",
    "Aquí, entregamos el contexto al LLM, y él nos responde a la _query_ con una explicación en lenguaje natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f57d87fcefdf3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe0699b4",
   "metadata": {},
   "source": [
    "## Parte 5: Generación de respuesta con OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad75d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3419098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(context: str, model=\"gpt-3.5-turbo\", temperature=0.2) -> str:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Eres un asistente experto en recuperación de información.\"},\n",
    "                {\"role\": \"user\", \"content\": context}\n",
    "            ],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error en la generación de respuesta: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d90936",
   "metadata": {},
   "source": [
    "## Parte 6: Bucle conversacional (chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88412c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "def ask_question(user_input: str, model=\"gpt-3.5-turbo\", temperature=0.2):\n",
    "\n",
    "    # Buscar documentos relevantes y preparar contexto\n",
    "    context, _ = run_search(user_input, faiss_index, normalized_embeddings, processed_documents)\n",
    "\n",
    "    # Generar respuesta con OpenAI\n",
    "    respuesta = generate_answer(context, model=model, temperature=temperature)\n",
    "\n",
    "    # Agregar al historial\n",
    "    conversation_history.append({\n",
    "        \"usuario\": user_input,\n",
    "        \"asistente\": respuesta\n",
    "    })\n",
    "\n",
    "    # Mostrar respuesta\n",
    "    print(f\"Asistente: {respuesta}\\n\")\n",
    "    return respuesta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0f0a424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asistente: El modelo BM25 fue propuesto por Stephen E. Robertson, Karen Spärck Jones y Steve Walker en 1995.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'El modelo BM25 fue propuesto por Stephen E. Robertson, Karen Spärck Jones y Steve Walker en 1995.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"¿Quien propuso BM25?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08a2fef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuario: ¿Qué es BM25?\n",
      "Asistente: Error en la generación de respuesta: No module named 'openai.resources'\n",
      "--------------------------------------------------\n",
      "Usuario: ¿Qué es BM25?\n",
      "Asistente: BM25 (Best Matching 25) es un modelo de ponderación de términos utilizado en la recuperación de información. Se utiliza para calcular el peso de los términos en un documento en función de su frecuencia y de la longitud del documento. El modelo BM25 tiene parámetros internos como b y k1 que deben ser ajustados para adaptarse a los datos y mejorar la precisión de la recuperación de información.\n",
      "--------------------------------------------------\n",
      "Usuario: ¿Quien propuso BM25?\n",
      "Asistente: El modelo BM25 fue propuesto por Stephen E. Robertson, Karen Spärck Jones y Steve Walker en 1995.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for t in conversation_history:\n",
    "    print(\"Usuario:\", t[\"usuario\"])\n",
    "    print(\"Asistente:\", t[\"asistente\"])\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
